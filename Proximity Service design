Step - 1 Requirement Gathering
Candidate: Can a user specify the search radius? If there are not enough businesses
within the search radius, does the system expand the search?
Interviewer: That's a great question. Let's assume we only care about businesses within
a specified radius. If time allows, we can then discuss how to expand the search if there
are not enough businesses within the radius,

Candidate: What's the maximal radius allowed? Can I assume it's 20km (12.5 miles
Interviewer: That's a reasonable assumption.
Candidate: Can a user change the search radius on the UI?
Interviewer: Yes, we have the following options: 0.5km (0.31 mile), 1km (0.62 mile).
2km (1.24 mile), 5km (3.1 mile), and 20km (12.42 mile).

Functional requirements
Based on this conversation, we focus on 3 key features:
• Return all businesses based on a user's location (latitude and longitude pair) and
radius.
• Business owners can add, delete or update a business, but this information doesn't
need to be reflected in real-time.
• Customers can view detailed information about a business.

Non-functional requirements
From the business requirements, we can infer a list of non-functional requirements. You
should also check these with the interviewer.
• Low latency, Users should be able to see nearby businesses quickly.
• Data privacy. Location info is sensitive data. When we design a location-based ser-
vice (LBS), we should always take user privacy into consideration. We need to com-
ply with data privacy laws like General Data Protection Regulation (GDPR) [4] and
California Consumer Privacy Act (CCPA) [5], etc.
• High availability and scalability requirements. We should ensure our sys

Back-of-the-envelope estimation
Let's take a look at some back-of-the-envelope calculations to determine the potential
scale and challenges our solution will need to address. Assume we have 100 million daily
active users and 200 million businesses.
Calculate QPS
• Seconds in a day = 24 × 60 × 60 = 86,400. We can round it up to 105 for easier
calculation. 105 is used throughout this book to represent seconds in a day.
• Assume a user makes 5 search queries per day.
100 million x 5
• Search QPS = -
105 - = 5,000

API design
GET /v1/search/nearby
Parameters
latitude Latitude of a given location decimal
longitude Longitude of a given location decimal
radius Optional. Default is 5000 meters (about 3 miles) int


APIS for business
ET /v1/businesses/:id Return detailed information about a business
POST /v1/businesses Add a business
PUT /v1/businesses/:id Update details of a business
DELETE /v1/businesses/:id Delete a business


business_id PK
address
city
state
country
latitude
longtitude
Table 1.3: Business table



Location-based service (LBS)
The LBS service is the core part of the system which finds nearby businesses for a given
radius and location. The LBS has the following characteristics:
• It is a read-heavy service with no write requests.
• QPS is high, especially during peak hours in dense areas.
• This service is stateless so it's easy to scale horizontally.


Algorithms to fetch nearby businesses
Option 1: Two-dimensional search

SELECT business_id, latitude, longitude,
FROM b u s i n e s s
WHERE (latitude BETWEEN {:my_lat} - radius AND {:my_lat} + radius)
(longitude BETWEEN {:my_long} - radius AND {:my_long} + radius)

This query is not efficient because we need to scan the whole table. What if we bul
indexes on longitude and latitude columns? Would this improve the efficiency? Tho
answer is not by much. The problem is that we have two-dimensional data and the database
returned from each dimension could still be huge.


Option 2: Evenly divided grid
Advantages
One simple approach is to evenly divide the world into small grids (Figure 1.6). This way,
one grid could have multiple businesses, and each business on the map belongs to one
grid.
Disadvantages
The main disadvantage is the "Distribution Problem," where fixed-size grids become inefficient by being either too crowded to filter quickly in dense cities or mostly empty in rural areas.



Option 3: Geo Hash
Geohash has 12 precisions (also called levels) as shown in Table 1.4. The precision factor
determines the size of the grid. We are only interested in geohashes with lengths between
4 and 6. This is because when it's longer than 6, the grid size is too small, while if it is
smaller than 4, the grid size is too large (see Table 14).

geohash length Grid width x height
(these are the size of grid)
1 5,009.4km x 4,992.6km (the size of the planet)
2 1,252.3km x 624.1km
3 156.5km x 156km
4 39.1km x 19.5km
5 4.9km x 4.9km
6 1.2km x 609.4m
7 152.9m x 152.4m
8 38.2m x 19m
9 4.8m x 4.8m
10 1.2m x 59.5cm
11 14.9cm x 14.9cm
12 3.7cm x 1.9cm


Radius (Kilometers) Geohash length
0.5km (0.31 mile) 6
1km (0.62 mile) 5
2km (1.24 mile) 5
5km (3.1 mile) 4
20km (12.42 mile) 4

Boundary issue
boundary issue is that two positions can have a long shared prefix, but they
belong to different geohashes as shown in Figure
A common solution is to fetch all businesses not only within the current grid but also
from its neighbors.


Not enough businesses
1: only return businesses within the radius. This option is easy to implement, but
the drawback is obvious. It doesn't return enough results to satisfy a user's needs.
2: increase the search radius. We can remove the last digit of the geohash and
use the new geohash to fetch nearby businesses. If there are not enough businesses,
we continue to expand the scope by removing another digit.


Option 4 Quad Tree
A quadtree [18] is a data structure that is com-
monly used to partition a two-dimensional space by recursively subdividing it into four
quadrants (grids) until the contents of the grids meet certain criteria. For example, the
criterion can be to keep subdividing until the number of businesses in the grid is not more
than 100.

Memory used by quad tree
The Memory Components

    Leaf Nodes: These store the actual business IDs (usually 8-byte integers) and their coordinates.

    Internal Nodes: These store 4 pointers to their "child" quadrants.

    The Metadata: Each node stores its own boundary (top, bottom, left, right coordinates).

1. The Leaf node  Data (Business Info)

   Business ID: 8 bytes

   Latitude (double): 8 bytes

   Longitude (double): 8 bytes

   Total per business: 24 bytes.

   For 200M businesses: 200,000,000×24 bytes≈4.8 GB.


2. The Tree Structure (The "Nodes")

Let's assume each leaf node holds 100 businesses.

    If we have 200M businesses, we have roughly 2 million leaf nodes.

    In a Quadtree, for every 4 leaf nodes, there is 1 parent node. The total number of nodes is roughly 4/3×(Number of Leaves).

    Total nodes ≈2.6 million nodes.

Each node needs:

    Boundary (4 doubles): 32 bytes

    Pointers to 4 children: 32 bytes

    Total per node: ~64 bytes.

    For 2.6M nodes: 2,600,000×64 bytes≈0.17 GB.

Total Memory Estimate

    Business Data: 4.8 GB

    Tree Structure: 0.17 GB

    Overhead (Buffer/Object headers): ~1 GB

    Grand Total: ~6 GB of RAM.

The real "cost" isn't the total size; it's the Update Cost. If 1 million food delivery drivers are moving their coordinates every second, the Quadtree has to constantly delete and re-insert nodes, which causes CPU spikes. This is why some systems use Geohashes for moving objects and Quadtrees for static businesses.
As mentioned above, it may take a few minutes to build a quadtree with 200 million busi-
nesses at the server start-up time.
Therefore, we should roll out a new release of the server incrementally to
a small subset of servers at a time.

GeoHash vs QuadTree
 When the precision (level) of geohash is fixed, the size of the grid is fixed as wel. It
cannot dynamically adjust the grid size, based on population density. More complex
logic is needed to support this.
Updating the index is easy. For example, to remove a business from the index.
Used when the locations are chaning

Quad Tree
 Slightly harder to implement because it needs to build the tree.
• Supports fetching k-nearest businesses. Sometimes we just want to return k-nearest
businesses and don't care if businesses are within a specified radius.
updates are

Design Deep Dive
The data for the business table may not all fit in one server, so it is a good candidate
for sharding. (consistent hashing)

Since our system is read heavy, we can introduce a cache
Earlier I thought we will use latitude and longitude as cache
Location coordinates returned from mobile phones are not accurate as they are just
the best estimation [32]. Even if you don't move, the results might be slightly differ-
ent each time you fetch coordinates on your phone.

we can use geoHash as the cache key and list of business Ids
When a new business is added, edited, or deleted, the database is updated and the cache
invalidated. Since the volume of those operations is relatively small and no locking mech-
anism is needed for the geohash approach, update operations are easy to deal with.

According to the requirements, a user can choose the following 4 radii on the client:
500m, 1km, 2km, and 5km. Those radii are mapped to geohash lengths of 4, 5, 5, and
6, respectively. To quickly fetch nearby businesses for different radii, we cache data in
Redis on all three precisions (geohash_4, geohash_5, and geohash_6).

As mentioned earlier, we have 200 million businesses and each business belongs to 1 grid
in a given precision. Therefore the total memory required is:
• Storage for Redis values: 8 bytes x 200 million x 3 precisions = ~ 5GB



In the end a small recap
just an example

Get nearby businesses
1. You try to find restaurants within 500 meters on Yelp. The client sends the user
location (latitude = 37.776720, longitude = -122.416730) and radius (500m) to the
load balancer.
2. The load balancer forwards the request to the LBS.
3. Based on the user location and radius info, the LBS finds the geohash length that
matches the search. By checking Table 1.5, 500m map to geohash length = 6.
4. LBS calculates neighboring geohashes and adds them to the list. The result looks
like this:
list_of_geohashes = [my_geohash, neighbor_geohash, neighbor2_geohash,
..., neighbor8_geohash]
5. For each geohash in list_of_geohashes, LBS calls the "Geohash" Redis server to
fetch corresponding business IDs. Calls to fetch business IDs for each geohash can
be made in parallel to reduce latency.
6. Based on the list of business IDs returned, LBS fetches fully hydrated business in-
formation from the "Business info" Redis server, then calculates distances between
a user and businesses, ranks them, and returns the result to the client.